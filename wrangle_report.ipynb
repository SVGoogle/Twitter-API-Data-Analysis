{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling report. Data analysis of Twitter API data\n",
    "\n",
    "This report is the summary of the workflow for the data wrangling process of Twitter user *WeRateDogs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Gathering Data\n",
    "\n",
    "The relevant data is retrieved by getting each of the three pieces of data as described below:\n",
    "\n",
    "1. The *WeRateDogs* Twitter archive, this file is downloaded manually by clicking the following link: [twitter_archive_enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv) and is stored as `df_archive`.\n",
    "\n",
    "2. The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file `image_predictions.tsv` is hosted on Udacity's servers. This file is stored as `df_predictions`.\n",
    "\n",
    "3. Additionaly, each tweet's retweet count and favorite (\"like\") count at minimum is gathered. Using the tweet IDs in the *WeRateDogs* Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called `tweet_json.txt` file. Each tweet's JSON data is written to its own line. Then this .txt file is read line by line into a pandas DataFrame. This file is stored as `df_stats`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Assessing Data\n",
    "\n",
    "After gathering each of the above pieces of data, they are assessed visually and programmatically for quality and tidiness issues. At least the following is detected and documented:\n",
    "   * (8) data quality issues,\n",
    "   * (2) data tidiness issues.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality assessment summary\n",
    "In `df_archive`:\n",
    "> 1. Columns `'doggo', 'floofer', 'pupper', 'puppo'` show that they have no missing values, however it is clear that 'None' in these columns are actual missing values that are not read in correctly by software. \n",
    "    * Furthermore, there are also a lot of values that are not register for any the dog stage category (all with 'None'), this means that the dog stage is also not categorized or mentioned in the tweet.\n",
    "    * And some observations have two different dog stages registered\n",
    "    * Columns `'doggo', 'floofer', 'pupper', 'puppo'` are of *object* type, could be converted to *category* type. However due to that there two dog stages in one observations, it will be kept as a string.\n",
    "\n",
    "> 2. Columns like `'tweet_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id'`are of *float64* or *int* type, must be converted to str type to be as no mathematical operations are not expected to perform. The same must be done for `'tweet_id'` in other DataFrames.\n",
    "\n",
    "> 3. Column `'timestamp'` is of *object* type, must be converted to *datetime* type. Furthermore, these dates are formatted differently than the `'created_at'` time data from Tweet Object.\n",
    "\n",
    "> 4. Validity issues with numerator and denominator data - some values are not extracted properly according to the rating system. Also both must be coverted to *float* type\n",
    "\n",
    "In `df_predictions` and `df_stats`:\n",
    "> 6. Number of rows in predictions and archive data don't match:\n",
    "    * There are some replies or retweets \n",
    "    * Also there are also some tweets without the dog rating\n",
    "    * Some tweets are expired and must be removed\n",
    "     \n",
    "> 7. Drop redundant columns like:\n",
    "    * `'source'` which just shows the utility used to post the Tweet. For example, Tweets from the Twitter website have a source value of web.\n",
    "    * `'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp' ` after filtering out the retweet and reply tweets\n",
    "    \n",
    "> 8. Filter the `df_predictions` and add only correct and highest predictions to the master file `df_master`\n",
    "    * Correct dog prediction label formatting\n",
    "    \n",
    "    \n",
    "\n",
    "**Additional comments** \n",
    "\n",
    "> Consistency of column names with the [Tweet Object](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object) attribute names in Tweet Data Dictionary. This may lead to confusion if someone else wants to extend the existing dataset. Rename the columns according to Tweet Object atributes:`'timestamp'` to `'created_at'` or with `'text'`  and `'full_text'`. However it is not implement in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidiness assessment summary\n",
    "\n",
    "1. In `df_archive` there are separate columns `'doggo', 'floofer', 'pupper', 'puppo'` that are one variable and must be melted together into a one column called `'dog_stage'`\n",
    "\n",
    "2. Merge into one dataset : Add the new retweet and favorite data from `df_stats` , valid predictions from `df_predictions` to twitter archive data in `df_archive` and create one master file `df_master` and store as `twitter-archive-master.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Cleaning Data\n",
    "\n",
    "The issues documented while assessing are cleaned in this part. Before conversion dataFrames are copied and renamed as follows:\n",
    "1. `df_clean`\n",
    "2. `df_predictions_clean`\n",
    "3. `df_stats_clean`\n",
    "\n",
    "\n",
    "## Saving Data\n",
    "\n",
    "The clean DataFrame `df_master` is saved in a CSV file named `twitter_archive_master.csv`. I have chosen to keep the 'False' predictions named 'Not a Dog' in the master file, however they will be filtered out during the analysis part. The same applies to the numerator values - they are also filtered out after loading the data again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV - Analyzing Data\n",
    "In this part the most popular dogs are determined in the following categories:\n",
    "1. Most popular Dog Breeds (**Escimo Dog** and **French Bulldog** based on mean retweet, favorite count)\n",
    "2. Most popular Dog Stage (**doggo** and **puppo** based on mean retweet, favorite count)\n",
    "3. Most frequent Dog Breeds submitted for rating (**Golden Retriever**, **Labrador Retriever** and **Pembroke** based on value counts)\n",
    "4. Highest rated Dog Breeds (**Golden Retriever**, **Pembroke** and **Samoyed**)\n",
    "\n",
    "Furthermore, most popular dog images from these insights are downloaded to folder named `Images`.\n",
    "\n",
    "Reporting of the project can be also found here:\n",
    "  1. Data wrangling code is reported in `wrangle_act.html`.\n",
    "  2. Summary of data analyses and visualizations are reported in `act_report.html`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
